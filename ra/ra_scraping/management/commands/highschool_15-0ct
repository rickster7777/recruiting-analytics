import requests
import re
from bs4 import BeautifulSoup
from urllib.request import urlopen
import csv
from datetime import datetime


now = datetime.now()
current_time = now.strftime("%H:%M:%S")
print("start Time =", current_time)

SITE = 'https://www.maxpreps.com'
POSITIONS = {"DB", "CB", "S", "SS","FS", "ATH"}

URL = requests.get('https://www.maxpreps.com/search/states_by_sport.aspx?gendersport=boys,football&season=fall')
SOUP = BeautifulSoup(URL.content, 'html.parser')

STATES = SOUP.findAll('div', {'class': 'states'})

ALL_STATES_LINK = STATES[0].find_all('a', href=True)

STATE_LIST = [state.text for state in ALL_STATES_LINK]

def maxpreps_page_parser(state):
    school_dict = {}
    for page in range(1,2):
        if state == 'district-of-columbia':
            rank_link = "https://www.maxpreps.com/rankings/football/{}/state/washington,-dc.htm".format(page)
        else:
            rank_link = "https://www.maxpreps.com/rankings/football/{}/state/{}.htm".format(page, state)
                
        rank_url = requests.get(rank_link)
        rank_soup = BeautifulSoup(rank_url.content, 'html.parser')
        try:
            table = rank_soup.find('table',{'class':'mx-grid sortable rankings-grid'})
            links = table.findAll('a')
        except AttributeError:
            pass            
        if links:
            links = links[1:]
            for each_link in links:
                school_dict[each_link.attrs['href'].split('/')[2]] = []
                #roaster_link = SITE + 'roster'.join(each_link.attrs['href'].split('home'))
                roaster_link = SITE + '/'.join(each_link.attrs['href'].split('/')[:-1]) + '/roster.htm'
                roaster_url = requests.get(roaster_link)
                roaster_soup = BeautifulSoup(roaster_url.content, 'html.parser')
                
                position_table = roaster_soup.findAll('table', {'id' : "roster"}) 
                if position_table:
                    position_table_body = position_table[0].find('tbody')
                    position_table_rows = position_table_body.find_all('tr')
                else:
                    break

                for row in position_table_rows:
                    cols = row.find_all(['td', 'th'])
                    try:
                        pos = set([item.strip() for item in cols[2].text.split(',')])
                    except IndexError:
                        pass  
                    
                    if set.intersection(POSITIONS, pos):
                        #school_dict[each_link.attrs['href'].split('/')[2]].append((cols[1].text, list(pos),cols[0].text,cols[4].text,cols[5].text))
                        profile_url = requests.get(cols[1].a['href'])
                        profile_soup = BeautifulSoup(profile_url.content, 'html.parser')
                        high_school = profile_soup.findAll('span', {'class':'school-name'}) 
                        #<span class="graduation-year">
                        class_grad = profile_soup.findAll('span',{'class':'graduation-year'})
                        try:
                            school_dict[each_link.attrs['href'].split('/')[2]].append((cols[1].text, list(pos),cols[0].text,cols[4].text,cols[5].text,class_grad[0].text.strip('Graduates in '),high_school[0].text))
                        except:
                            pass    
                        

    return school_dict

def main_parser():
    final_data = {}
    state_copy = {}
    iterate_states = []
   
    iterate_states = STATE_LIST.pop() 

    for state in STATE_LIST:       
        final_data[state] = maxpreps_page_parser(state.replace(' ','-').lower())
        for k,val in final_data[state].items():
            obj = []
            split_par = k.replace('(',' ').replace(')', ' ')
            sch_cit_stat = split_par.replace(',', ' ').split()
            for v in val:
                name = v[0]                
                pos = str(v[1]).replace('[','').replace(']', '').replace('\'','')
                jersey_no = v[2]
                height = v[3]
                weight = v[4]
                obj.append(sch_cit_stat[0].replace('-', ' ').strip())
                obj.append(sch_cit_stat[1].replace('-',' ').strip())
                obj.append(sch_cit_stat[2])
                obj.append(name)
                obj.append(pos)
                obj.append(jersey_no)
                obj.append(height)
                obj.append(weight)
                with open('/home/rishavk/Desktop/ra_repo/RA Scraping/players4.csv', 'a') as toWrite:
                    writer = csv.writer(toWrite)                        
                    writer.writerow(obj)
                #print("'{}' data added successfully".format(name))    
                obj=[]                      
if __name__ == "__main__":
    main_parser()


print("End Time =", current_time)
